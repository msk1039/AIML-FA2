{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7aaf24d",
   "metadata": {},
   "source": [
    "# JEE Cutoff Prediction Model\n",
    "## Phase 3: Model Building and Training\n",
    "\n",
    "**Objective**: Build and train XGBoost model to predict JEE cutoffs\n",
    "\n",
    "**Key Tasks**:\n",
    "- Load feature-engineered data\n",
    "- Split data using time-series approach\n",
    "- Train XGBoost model\n",
    "- Hyperparameter tuning\n",
    "- Evaluate model performance\n",
    "- Analyze feature importance\n",
    "\n",
    "**Course Alignment**: \n",
    "- Unit 4: Regression\n",
    "- Unit 5: Ensemble Learning (XGBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2fb419",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ca67cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully!\n",
      "XGBoost version: 3.1.1\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2599fd",
   "metadata": {},
   "source": [
    "## Step 2: Load Feature-Engineered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e85c1350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data loaded successfully!\n",
      "üìä Shape: 79521 rows √ó 25 columns\n",
      "\n",
      "First few rows:\n",
      "   institute_encoded  branch_encoded  quota_encoded  seat_type_encoded  \\\n",
      "0                  0               1              0                  0   \n",
      "1                  0               1              0                  0   \n",
      "2                  0               1              0                  0   \n",
      "3                  0               1              0                  0   \n",
      "4                  0               1              0                  0   \n",
      "\n",
      "   gender_encoded  branch_demand_category_encoded  cutoff_prev_1yr  \\\n",
      "0               1                               1          54330.8   \n",
      "1               1                               1           7839.0   \n",
      "2               1                               1          11172.0   \n",
      "3               1                               1          13870.0   \n",
      "4               1                               1          13007.0   \n",
      "\n",
      "   cutoff_prev_2yr  cutoff_prev_3yr  cutoff_mean_3yr  cutoff_std_3yr  \\\n",
      "0      4655.428571      4286.571429     51749.114286        0.000000   \n",
      "1     51241.250000      4534.062500      7839.000000        0.000000   \n",
      "2      7839.000000     49844.151515      9505.500000     2356.786902   \n",
      "3     11172.000000      7839.000000     10960.333333     3021.066423   \n",
      "4     13870.000000     11172.000000     12683.000000     1377.872636   \n",
      "\n",
      "   cutoff_change_1yr  cutoff_pct_change_1yr  institute_avg_cutoff  \\\n",
      "0                0.0                   0.00          62436.230769   \n",
      "1                0.0                   0.00          48305.788462   \n",
      "2             3333.0                  42.52          46081.740000   \n",
      "3             2698.0                  24.15          38521.641509   \n",
      "4             -863.0                  -6.22          38886.918367   \n",
      "\n",
      "   institute_tier  branch_avg_cutoff  institute_branch_avg  \\\n",
      "0               2       51749.114286               69722.9   \n",
      "1               2       31228.897436               69722.9   \n",
      "2               2       29650.307692               69722.9   \n",
      "3               2       26363.717949               69722.9   \n",
      "4               2       26330.289474               69722.9   \n",
      "\n",
      "   institute_branch_vs_avg  year  years_since_start  is_recent   cutoff  \\\n",
      "0            -17973.785714  2019                  1          0   7839.0   \n",
      "1            -38494.002564  2020                  2          0  11172.0   \n",
      "2            -40072.592308  2021                  3          0  13870.0   \n",
      "3            -43359.182051  2022                  4          1  13007.0   \n",
      "4            -43392.610526  2023                  5          1  13631.0   \n",
      "\n",
      "                                             seat_id  \\\n",
      "0  Assam University, Silchar_AG_AI_EWS_Gender-Neu...   \n",
      "1  Assam University, Silchar_AG_AI_EWS_Gender-Neu...   \n",
      "2  Assam University, Silchar_AG_AI_EWS_Gender-Neu...   \n",
      "3  Assam University, Silchar_AG_AI_EWS_Gender-Neu...   \n",
      "4  Assam University, Silchar_AG_AI_EWS_Gender-Neu...   \n",
      "\n",
      "                   institute branch  \n",
      "0  Assam University, Silchar     AG  \n",
      "1  Assam University, Silchar     AG  \n",
      "2  Assam University, Silchar     AG  \n",
      "3  Assam University, Silchar     AG  \n",
      "4  Assam University, Silchar     AG  \n",
      "\n",
      "üìã Total features for modeling: 21\n",
      "\n",
      "‚úì Feature list loaded from Phase 2\n"
     ]
    }
   ],
   "source": [
    "# Load the model-ready dataset\n",
    "df = pd.read_csv('cutoffs_model_ready.csv')\n",
    "\n",
    "print(\"‚úÖ Data loaded successfully!\")\n",
    "print(f\"üìä Shape: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Load feature names\n",
    "feature_info = pd.read_csv('feature_names.csv')\n",
    "feature_columns = feature_info['feature_name'].tolist()\n",
    "\n",
    "print(f\"\\nüìã Total features for modeling: {len(feature_columns)}\")\n",
    "print(f\"\\n‚úì Feature list loaded from Phase 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8e6f5c",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Features and Target\n",
    "\n",
    "Separate features (X) and target variable (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e88b223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Features and target prepared!\n",
      "\n",
      "üìä Feature matrix (X) shape: (79521, 21)\n",
      "üìä Target vector (y) shape: (79521,)\n",
      "\n",
      "üìà Target variable statistics:\n",
      "count    7.952100e+04\n",
      "mean     1.930295e+04\n",
      "std      4.066367e+05\n",
      "min      1.000000e+00\n",
      "25%      1.397000e+03\n",
      "50%      4.587000e+03\n",
      "75%      1.300800e+04\n",
      "max      1.008650e+08\n",
      "Name: cutoff, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Define features and target\n",
    "X = df[feature_columns].copy()\n",
    "y = df['cutoff'].copy()\n",
    "\n",
    "# Keep metadata for later analysis\n",
    "metadata = df[['seat_id', 'institute', 'branch']].copy()\n",
    "\n",
    "print(\"‚úÖ Features and target prepared!\")\n",
    "print(f\"\\nüìä Feature matrix (X) shape: {X.shape}\")\n",
    "print(f\"üìä Target vector (y) shape: {y.shape}\")\n",
    "print(f\"\\nüìà Target variable statistics:\")\n",
    "print(y.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244eced1",
   "metadata": {},
   "source": [
    "## Step 4: Extract Year Information\n",
    "\n",
    "Get the year column for time-series splitting (must be done before train-test split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f226194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Year information extracted!\n",
      "\n",
      "üìä Year distribution:\n",
      "year\n",
      "2018     7157\n",
      "2019     8745\n",
      "2020     9340\n",
      "2021     9452\n",
      "2022    10023\n",
      "2023    10842\n",
      "2024    11688\n",
      "2025    12274\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get year column for splitting (save before any data manipulation)\n",
    "years = df['year'].copy()\n",
    "\n",
    "print(\"‚úÖ Year information extracted!\")\n",
    "print(f\"\\nüìä Year distribution:\")\n",
    "print(years.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39e170c",
   "metadata": {},
   "source": [
    "## Step 5: Check for Missing Values and Handle Them\n",
    "\n",
    "Before training, we need to check and handle any missing values in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdf2bda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking for missing values...\n",
      "\n",
      "================================================================================\n",
      "üìä MISSING VALUES IN DATASET\n",
      "================================================================================\n",
      "\n",
      "‚ö†Ô∏è  Found 3 features with missing values:\n",
      "  cutoff_prev_1yr: 125 missing (0.16%)\n",
      "  cutoff_prev_2yr: 417 missing (0.52%)\n",
      "  cutoff_prev_3yr: 978 missing (1.23%)\n",
      "\n",
      "üîÑ Handling missing values...\n",
      "  ‚úì Filled cutoff_prev_1yr with median: 5336.47\n",
      "  ‚úì Filled cutoff_prev_2yr with median: 5812.23\n",
      "  ‚úì Filled cutoff_prev_3yr with median: 5643.00\n",
      "\n",
      "‚úÖ Missing values handled!\n",
      "  Before: 1,520 missing values\n",
      "  After: 0 missing values\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in the entire dataset\n",
    "print(\"üîç Checking for missing values...\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä MISSING VALUES IN DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "missing_before = X.isnull().sum()\n",
    "missing_features = missing_before[missing_before > 0]\n",
    "\n",
    "if len(missing_features) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  Found {len(missing_features)} features with missing values:\")\n",
    "    for feature, count in missing_features.items():\n",
    "        pct = (count / len(X)) * 100\n",
    "        print(f\"  {feature}: {count:,} missing ({pct:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nüîÑ Handling missing values...\")\n",
    "    \n",
    "    # Fill missing values with median for numeric features\n",
    "    # This is safe for our use case (lag features, statistical features)\n",
    "    for col in missing_features.index:\n",
    "        median_value = X[col].median()\n",
    "        X[col].fillna(median_value, inplace=True)\n",
    "        print(f\"  ‚úì Filled {col} with median: {median_value:.2f}\")\n",
    "    \n",
    "    # Verify no more missing values\n",
    "    missing_after = X.isnull().sum().sum()\n",
    "    print(f\"\\n‚úÖ Missing values handled!\")\n",
    "    print(f\"  Before: {missing_before.sum():,} missing values\")\n",
    "    print(f\"  After: {missing_after} missing values\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No missing values found!\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e099ffef",
   "metadata": {},
   "source": [
    "## Step 6: Time-Series Train-Test Split\n",
    "\n",
    "**Important**: We use time-based split, NOT random split!\n",
    "- Train: 2018-2023 (6 years)\n",
    "- Test: 2024 (1 year)\n",
    "\n",
    "This prevents data leakage (using future data to predict past)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f177be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Time-series split completed!\n",
      "\n",
      "üìä Training set:\n",
      "  - Years: 2018 to 2023\n",
      "  - Samples: 55,559\n",
      "  - Shape: (55559, 21)\n",
      "\n",
      "üìä Test set:\n",
      "  - Year: 2024\n",
      "  - Samples: 11,688\n",
      "  - Shape: (11688, 21)\n",
      "\n",
      "üìà Train-Test split ratio: 69.9% - 14.7%\n",
      "\n",
      "üîç Final data quality check:\n",
      "  - NaN in X_train: 0\n",
      "  - NaN in X_test: 0\n",
      "  - NaN in y_train: 0\n",
      "  - NaN in y_test: 0\n"
     ]
    }
   ],
   "source": [
    "# Time-based split: Train on 2018-2023, Test on 2024\n",
    "train_mask = years < 2024\n",
    "test_mask = years == 2024\n",
    "\n",
    "X_train = X[train_mask].copy()\n",
    "y_train = y[train_mask].copy()\n",
    "X_test = X[test_mask].copy()\n",
    "y_test = y[test_mask].copy()\n",
    "\n",
    "# Metadata for analysis\n",
    "train_metadata = metadata[train_mask].copy()\n",
    "test_metadata = metadata[test_mask].copy()\n",
    "\n",
    "print(\"‚úÖ Time-series split completed!\")\n",
    "print(f\"\\nüìä Training set:\")\n",
    "print(f\"  - Years: {years[train_mask].min()} to {years[train_mask].max()}\")\n",
    "print(f\"  - Samples: {len(X_train):,}\")\n",
    "print(f\"  - Shape: {X_train.shape}\")\n",
    "\n",
    "print(f\"\\nüìä Test set:\")\n",
    "print(f\"  - Year: {years[test_mask].unique()[0]}\")\n",
    "print(f\"  - Samples: {len(X_test):,}\")\n",
    "print(f\"  - Shape: {X_test.shape}\")\n",
    "\n",
    "print(f\"\\nüìà Train-Test split ratio: {len(X_train)/len(X)*100:.1f}% - {len(X_test)/len(X)*100:.1f}%\")\n",
    "\n",
    "# Final check for NaN values in train and test sets\n",
    "print(f\"\\nüîç Final data quality check:\")\n",
    "print(f\"  - NaN in X_train: {X_train.isnull().sum().sum()}\")\n",
    "print(f\"  - NaN in X_test: {X_test.isnull().sum().sum()}\")\n",
    "print(f\"  - NaN in y_train: {y_train.isnull().sum()}\")\n",
    "print(f\"  - NaN in y_test: {y_test.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069efe15",
   "metadata": {},
   "source": [
    "## Step 7: Baseline Model - Simple Linear Regression\n",
    "\n",
    "Create a baseline to compare XGBoost performance against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "100eec87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Training baseline Linear Regression model...\n",
      "\n",
      "‚úÖ Baseline model trained!\n",
      "\n",
      "================================================================================\n",
      "üìä BASELINE MODEL PERFORMANCE (Linear Regression)\n",
      "================================================================================\n",
      "\n",
      "Training Set:\n",
      "  MAE:  24,254.53 ranks\n",
      "  RMSE: 438,059.58 ranks\n",
      "  R¬≤:   0.0541\n",
      "\n",
      "Test Set (2024):\n",
      "  MAE:  27,169.44 ranks\n",
      "  RMSE: 263,666.55 ranks\n",
      "  R¬≤:   0.1922\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "print(\"üîÑ Training baseline Linear Regression model...\")\n",
    "\n",
    "# Train simple linear regression\n",
    "baseline_model = LinearRegression()\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_baseline = baseline_model.predict(X_train)\n",
    "y_test_pred_baseline = baseline_model.predict(X_test)\n",
    "\n",
    "# Evaluate baseline\n",
    "train_mae_baseline = mean_absolute_error(y_train, y_train_pred_baseline)\n",
    "test_mae_baseline = mean_absolute_error(y_test, y_test_pred_baseline)\n",
    "train_rmse_baseline = np.sqrt(mean_squared_error(y_train, y_train_pred_baseline))\n",
    "test_rmse_baseline = np.sqrt(mean_squared_error(y_test, y_test_pred_baseline))\n",
    "train_r2_baseline = r2_score(y_train, y_train_pred_baseline)\n",
    "test_r2_baseline = r2_score(y_test, y_test_pred_baseline)\n",
    "\n",
    "print(\"\\n‚úÖ Baseline model trained!\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä BASELINE MODEL PERFORMANCE (Linear Regression)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTraining Set:\")\n",
    "print(f\"  MAE:  {train_mae_baseline:,.2f} ranks\")\n",
    "print(f\"  RMSE: {train_rmse_baseline:,.2f} ranks\")\n",
    "print(f\"  R¬≤:   {train_r2_baseline:.4f}\")\n",
    "\n",
    "print(f\"\\nTest Set (2024):\")\n",
    "print(f\"  MAE:  {test_mae_baseline:,.2f} ranks\")\n",
    "print(f\"  RMSE: {test_rmse_baseline:,.2f} ranks\")\n",
    "print(f\"  R¬≤:   {test_r2_baseline:.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91d722f",
   "metadata": {},
   "source": [
    "## Step 8: Build Initial XGBoost Model\n",
    "\n",
    "Train XGBoost with default parameters first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b283512d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Training initial XGBoost model...\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "XGBModel.fit() got an unexpected keyword argument 'eval_metric'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      4\u001b[39m xgb_initial = xgb.XGBRegressor(\n\u001b[32m      5\u001b[39m     objective=\u001b[33m'\u001b[39m\u001b[33mreg:squarederror\u001b[39m\u001b[33m'\u001b[39m,  \u001b[38;5;66;03m# Regression task\u001b[39;00m\n\u001b[32m      6\u001b[39m     n_estimators=\u001b[32m100\u001b[39m,               \u001b[38;5;66;03m# Number of trees\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m     n_jobs=-\u001b[32m1\u001b[39m                       \u001b[38;5;66;03m# Use all CPU cores\u001b[39;00m\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mxgb_initial\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmae\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     20\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Predictions\u001b[39;00m\n\u001b[32m     23\u001b[39m y_train_pred_initial = xgb_initial.predict(X_train)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.13/site-packages/xgboost/core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: XGBModel.fit() got an unexpected keyword argument 'eval_metric'"
     ]
    }
   ],
   "source": [
    "print(\"üîÑ Training initial XGBoost model...\\n\")\n",
    "\n",
    "# Create XGBoost regressor with initial parameters\n",
    "xgb_initial = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',  # Regression task\n",
    "    n_estimators=100,               # Number of trees\n",
    "    max_depth=6,                    # Tree depth\n",
    "    learning_rate=0.1,              # Learning rate\n",
    "    random_state=42,\n",
    "    n_jobs=-1,                      # Use all CPU cores\n",
    "    eval_metric='mae'               # Evaluation metric\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_initial.fit(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_initial = xgb_initial.predict(X_train)\n",
    "y_test_pred_initial = xgb_initial.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "train_mae_initial = mean_absolute_error(y_train, y_train_pred_initial)\n",
    "test_mae_initial = mean_absolute_error(y_test, y_test_pred_initial)\n",
    "train_rmse_initial = np.sqrt(mean_squared_error(y_train, y_train_pred_initial))\n",
    "test_rmse_initial = np.sqrt(mean_squared_error(y_test, y_test_pred_initial))\n",
    "train_r2_initial = r2_score(y_train, y_train_pred_initial)\n",
    "test_r2_initial = r2_score(y_test, y_test_pred_initial)\n",
    "\n",
    "print(\"‚úÖ Initial XGBoost model trained!\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä INITIAL XGBOOST PERFORMANCE (Default Parameters)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTraining Set:\")\n",
    "print(f\"  MAE:  {train_mae_initial:,.2f} ranks\")\n",
    "print(f\"  RMSE: {train_rmse_initial:,.2f} ranks\")\n",
    "print(f\"  R¬≤:   {train_r2_initial:.4f}\")\n",
    "\n",
    "print(f\"\\nTest Set (2024):\")\n",
    "print(f\"  MAE:  {test_mae_initial:,.2f} ranks\")\n",
    "print(f\"  RMSE: {test_rmse_initial:,.2f} ranks\")\n",
    "print(f\"  R¬≤:   {test_r2_initial:.4f}\")\n",
    "\n",
    "print(f\"\\nüí° Improvement over baseline:\")\n",
    "print(f\"  MAE improvement: {test_mae_baseline - test_mae_initial:,.2f} ranks ({(test_mae_baseline - test_mae_initial)/test_mae_baseline*100:.1f}%)\")\n",
    "print(f\"  RMSE improvement: {test_rmse_baseline - test_rmse_initial:,.2f} ranks ({(test_rmse_baseline - test_rmse_initial)/test_rmse_baseline*100:.1f}%)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc670218",
   "metadata": {},
   "source": [
    "## Step 9: Hyperparameter Tuning\n",
    "\n",
    "Use RandomizedSearchCV to find optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af77d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ Starting hyperparameter tuning...\\n\")\n",
    "print(\"‚è≥ This may take several minutes...\\n\")\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'max_depth': [3, 4, 5, 6, 7],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.15],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'gamma': [0, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# Create base model\n",
    "xgb_base = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Time series cross-validation\n",
    "# We'll use 3 splits on training data\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "# Randomized search (faster than GridSearch)\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_base,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=30,  # Try 30 random combinations\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    cv=tscv,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the random search\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n‚úÖ Hyperparameter tuning completed!\")\n",
    "print(f\"\\nüèÜ Best parameters found:\")\n",
    "for param, value in random_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\nüìä Best cross-validation MAE: {-random_search.best_score_:,.2f} ranks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10b9ac4",
   "metadata": {},
   "source": [
    "## Step 10: Train Final Optimized Model\n",
    "\n",
    "Train XGBoost with the best hyperparameters found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd331e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ Training final optimized XGBoost model...\\n\")\n",
    "\n",
    "# Get the best model from random search\n",
    "xgb_final = random_search.best_estimator_\n",
    "\n",
    "# Set eval_metric for the final model\n",
    "xgb_final.set_params(eval_metric='mae')\n",
    "\n",
    "# Train on full training data with evaluation sets\n",
    "xgb_final.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_final = xgb_final.predict(X_train)\n",
    "y_test_pred_final = xgb_final.predict(X_test)\n",
    "\n",
    "# Evaluate final model\n",
    "train_mae_final = mean_absolute_error(y_train, y_train_pred_final)\n",
    "test_mae_final = mean_absolute_error(y_test, y_test_pred_final)\n",
    "train_rmse_final = np.sqrt(mean_squared_error(y_train, y_train_pred_final))\n",
    "test_rmse_final = np.sqrt(mean_squared_error(y_test, y_test_pred_final))\n",
    "train_r2_final = r2_score(y_train, y_train_pred_final)\n",
    "test_r2_final = r2_score(y_test, y_test_pred_final)\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error)\n",
    "train_mape_final = np.mean(np.abs((y_train - y_train_pred_final) / y_train)) * 100\n",
    "test_mape_final = np.mean(np.abs((y_test - y_test_pred_final) / y_test)) * 100\n",
    "\n",
    "print(\"‚úÖ Final optimized model trained!\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä FINAL OPTIMIZED XGBOOST PERFORMANCE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTraining Set:\")\n",
    "print(f\"  MAE:   {train_mae_final:,.2f} ranks\")\n",
    "print(f\"  RMSE:  {train_rmse_final:,.2f} ranks\")\n",
    "print(f\"  R¬≤:    {train_r2_final:.4f}\")\n",
    "print(f\"  MAPE:  {train_mape_final:.2f}%\")\n",
    "\n",
    "print(f\"\\nTest Set (2024):\")\n",
    "print(f\"  MAE:   {test_mae_final:,.2f} ranks\")\n",
    "print(f\"  RMSE:  {test_rmse_final:,.2f} ranks\")\n",
    "print(f\"  R¬≤:    {test_r2_final:.4f}\")\n",
    "print(f\"  MAPE:  {test_mape_final:.2f}%\")\n",
    "\n",
    "print(f\"\\nüí° Improvement over baseline (Linear Regression):\")\n",
    "print(f\"  MAE improvement:  {test_mae_baseline - test_mae_final:,.2f} ranks ({(test_mae_baseline - test_mae_final)/test_mae_baseline*100:.1f}%)\")\n",
    "print(f\"  RMSE improvement: {test_rmse_baseline - test_rmse_final:,.2f} ranks ({(test_rmse_baseline - test_rmse_final)/test_rmse_baseline*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüí° Improvement over initial XGBoost:\")\n",
    "print(f\"  MAE improvement:  {test_mae_initial - test_mae_final:,.2f} ranks ({(test_mae_initial - test_mae_final)/test_mae_initial*100:.1f}%)\")\n",
    "print(f\"  RMSE improvement: {test_rmse_initial - test_rmse_final:,.2f} ranks ({(test_rmse_initial - test_rmse_final)/test_rmse_initial*100:.1f}%)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59baf47f",
   "metadata": {},
   "source": [
    "## Step 11: Feature Importance Analysis\n",
    "\n",
    "Understand which features contribute most to predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ca0d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': xgb_final.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"üìä TOP 15 MOST IMPORTANT FEATURES\")\n",
    "print(\"=\"*80)\n",
    "print(feature_importance.head(15).to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Importance Score', fontsize=12)\n",
    "plt.title('Top 15 Feature Importance in XGBoost Model', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Feature importance analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703e817f",
   "metadata": {},
   "source": [
    "## Step 12: Prediction Analysis - Actual vs Predicted\n",
    "\n",
    "Visualize how well the model predicts cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d67102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Actual vs Predicted (Test Set)\n",
    "axes[0].scatter(y_test, y_test_pred_final, alpha=0.5, s=10)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual Cutoff Rank', fontsize=12)\n",
    "axes[0].set_ylabel('Predicted Cutoff Rank', fontsize=12)\n",
    "axes[0].set_title('Actual vs Predicted Cutoffs (Test Set 2024)', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Residual plot\n",
    "residuals = y_test - y_test_pred_final\n",
    "axes[1].scatter(y_test_pred_final, residuals, alpha=0.5, s=10)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[1].set_xlabel('Predicted Cutoff Rank', fontsize=12)\n",
    "axes[1].set_ylabel('Residuals (Actual - Predicted)', fontsize=12)\n",
    "axes[1].set_title('Residual Plot', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Prediction visualization completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37d875f",
   "metadata": {},
   "source": [
    "## Step 13: Error Analysis by Category\n",
    "\n",
    "Analyze model performance across different branches and institute tiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7520ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results dataframe for test set\n",
    "test_results = test_metadata.copy()\n",
    "test_results['actual'] = y_test.values\n",
    "test_results['predicted'] = y_test_pred_final\n",
    "test_results['error'] = np.abs(y_test.values - y_test_pred_final)\n",
    "test_results['pct_error'] = (test_results['error'] / test_results['actual'] * 100)\n",
    "\n",
    "print(\"üìä ERROR ANALYSIS BY BRANCH\")\n",
    "print(\"=\"*80)\n",
    "branch_errors = test_results.groupby('branch').agg({\n",
    "    'error': ['mean', 'median', 'std'],\n",
    "    'pct_error': 'mean',\n",
    "    'actual': 'count'\n",
    "}).round(2)\n",
    "branch_errors.columns = ['MAE', 'Median Error', 'Std Error', 'MAPE %', 'Count']\n",
    "branch_errors = branch_errors.sort_values('MAE', ascending=False).head(10)\n",
    "print(branch_errors)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä BEST PERFORMING BRANCHES (Lowest Error)\")\n",
    "print(\"=\"*80)\n",
    "best_branches = test_results.groupby('branch').agg({\n",
    "    'error': 'mean',\n",
    "    'actual': 'count'\n",
    "}).round(2)\n",
    "best_branches.columns = ['MAE', 'Count']\n",
    "best_branches = best_branches[best_branches['Count'] >= 10].sort_values('MAE').head(10)\n",
    "print(best_branches)\n",
    "\n",
    "print(\"\\n‚úÖ Error analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14aff92",
   "metadata": {},
   "source": [
    "## Step 14: Sample Predictions\n",
    "\n",
    "Show some example predictions for different institutes and branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894a32d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample predictions\n",
    "sample_results = test_results[['institute', 'branch', 'actual', 'predicted', 'error', 'pct_error']].copy()\n",
    "\n",
    "print(\"üìã SAMPLE PREDICTIONS (Best Predictions)\")\n",
    "print(\"=\"*80)\n",
    "print(sample_results.nsmallest(10, 'error').to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã SAMPLE PREDICTIONS (Worst Predictions)\")\n",
    "print(\"=\"*80)\n",
    "print(sample_results.nlargest(10, 'error').to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã RANDOM SAMPLE PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "print(sample_results.sample(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2523c1",
   "metadata": {},
   "source": [
    "## Step 15: Save the Trained Model\n",
    "\n",
    "Save the model for future predictions and deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57170fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "with open('xgboost_cutoff_model.pkl', 'wb') as f:\n",
    "    pickle.dump(xgb_final, f)\n",
    "\n",
    "print(\"‚úÖ Model saved successfully!\")\n",
    "print(f\"üìÅ File: xgboost_cutoff_model.pkl\")\n",
    "\n",
    "# Save model performance metrics\n",
    "performance_metrics = {\n",
    "    'model': 'XGBoost Regressor',\n",
    "    'training_years': '2018-2023',\n",
    "    'test_year': '2024',\n",
    "    'test_mae': test_mae_final,\n",
    "    'test_rmse': test_rmse_final,\n",
    "    'test_r2': test_r2_final,\n",
    "    'test_mape': test_mape_final,\n",
    "    'n_features': len(feature_columns),\n",
    "    'best_params': random_search.best_params_\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('model_performance.json', 'w') as f:\n",
    "    json.dump(performance_metrics, f, indent=4)\n",
    "\n",
    "print(f\"\\n‚úÖ Performance metrics saved!\")\n",
    "print(f\"üìÅ File: model_performance.json\")\n",
    "\n",
    "# Save feature importance\n",
    "feature_importance.to_csv('feature_importance.csv', index=False)\n",
    "print(f\"\\n‚úÖ Feature importance saved!\")\n",
    "print(f\"üìÅ File: feature_importance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989ea54f",
   "metadata": {},
   "source": [
    "## Step 16: Model Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd3e5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ PHASE 3: MODEL BUILDING COMPLETED!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä FINAL MODEL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model Type: XGBoost Regressor (Ensemble Learning)\")\n",
    "print(f\"Training Data: 2018-2023 ({len(X_train):,} samples)\")\n",
    "print(f\"Test Data: 2024 ({len(X_test):,} samples)\")\n",
    "print(f\"Number of Features: {len(feature_columns)}\")\n",
    "\n",
    "print(f\"\\nüèÜ BEST HYPERPARAMETERS:\")\n",
    "for param, value in random_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\nüìà PERFORMANCE METRICS (Test Set 2024):\")\n",
    "print(f\"  Mean Absolute Error (MAE):  {test_mae_final:,.2f} ranks\")\n",
    "print(f\"  Root Mean Squared Error:    {test_rmse_final:,.2f} ranks\")\n",
    "print(f\"  R¬≤ Score:                   {test_r2_final:.4f}\")\n",
    "print(f\"  Mean Absolute % Error:      {test_mape_final:.2f}%\")\n",
    "\n",
    "print(f\"\\nüí° INTERPRETATION:\")\n",
    "print(f\"  - On average, predictions are off by {test_mae_final:,.0f} ranks\")\n",
    "print(f\"  - Model explains {test_r2_final*100:.1f}% of variance in cutoffs\")\n",
    "print(f\"  - Percentage error is {test_mape_final:.1f}% on average\")\n",
    "\n",
    "print(f\"\\nüîù TOP 5 MOST IMPORTANT FEATURES:\")\n",
    "for i, row in feature_importance.head(5).iterrows():\n",
    "    print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "print(f\"\\nüìÅ FILES CREATED:\")\n",
    "print(f\"  1. xgboost_cutoff_model.pkl - Trained XGBoost model\")\n",
    "print(f\"  2. model_performance.json - Performance metrics\")\n",
    "print(f\"  3. feature_importance.csv - Feature importance scores\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ Ready for Phase 4: Making Predictions!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Global .venv)",
   "language": "python",
   "name": "global-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
